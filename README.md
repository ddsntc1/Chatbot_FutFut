# Chatbot_FutFut

### About FutFut
<img src="https://github.com/ddsntc1/Chatbot_FutFut/assets/38596856/cb1cd8b7-c556-46a8-ab8d-e093af713433.jpg" width="400" height="400">

- Domain : í’‹ì‚´ í”Œë«í¼ 
- Concept : 'í•´ìš”'ì²´ë¥¼ ì‚¬ìš©í•˜ë©° ì¹œì ˆí•˜ê²Œ ë‹µí•˜ëŠ” ì±—ë´‡. ë§ëì— 'ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”! í’‹í’‹~!'ì„ ë¶™ì—¬ í’‹í’‹ì´ ì»¨ì…‰ì„ ìœ ì§€ 
- Model : 
- Dataset :
  
  [Dongwookss/q_a_korean_futsal](https://huggingface.co/datasets/Dongwookss/q_a_korean_futsal)
  
  [mintaeng/llm_futsaldata_yo](https://huggingface.co/datasets/mintaeng/llm_futsaldata_yo)
- How-to? LLM fine-tuning & RAG 
---
### How to use? 
- Fine-tuned Model : Llama3-8b, Zephyr-7b

<details>
  <summary>Scripts Description</summary>
  
   load_model_type_a.py : transformersì˜ AutoModelForCausalLMì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
    
   load_model_type_b.py : Unsloth íŒ¨í‚¤ì§€ì˜ FastLanguageModelì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ íŠœë‹ì„ ìœ„í•œ íŒ¨í‚¤ì§€ì´ê¸° ë•Œë¬¸ì— Huggingfaceì— adapter_configê°€ ì¡´ì¬í•˜ë©´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í•©ë‹ˆë‹¤.

   main.py : packì— ìˆëŠ” ëª¨ë“ˆì„ í™œìš©í•˜ì—¬ Fine-tuned Modelì„ ë¶ˆëŸ¬ì˜¤ê³  RAGë¥¼ ì ìš©ì‹œì¼œ FastAPIë¡œ ìš”ì²­ê³¼ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

   ```python
  uvicorn main:app --reload -p <í¬íŠ¸ë²ˆí˜¸ì§€ì •>
  ```

</details>

  
---


#### About Fine-tuning

- Method : By Unsloth
- Trainer : SFTrainer, ~~DPOTrainer~~

#### About RAG

- Method

#### Fine-tuned Result(HuggingFaceğŸ¤—): 


- [Dongwooks](https://huggingface.co/Dongwookss)


- [mintaeng](https://huggingface.co/mintaeng)

- [bigr00t](https://huggingface.co/bigr00t)

<details>
  <summary>Using HuggingFace Model with out RAG </summary>
  
``` python
# Using HuggingFace Model with out RAG 
# !pip install transformers==4.40.0 accelerate

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import TextStreamer

model_id = 'Dongwookss/ì›í•˜ëŠ”ëª¨ë¸'
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

PROMPT = '''
Below is an instruction that describes a task. Write a response that appropriately completes the request.
'''
instruction = "question"

messages = [
    {"role": "system", "content": f"{PROMPT}"},
    {"role": "user", "content": f"{instruction}"}
    ]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

text_streamer = TextStreamer(tokenizer)
output = model.generate(
    input_ids,
    max_new_tokens=4096,
    eos_token_id=terminators,
    do_sample=True,
    streamer = text_streamer,
    temperature=0.6,
    top_p=0.9,
    repetition_penalty = 1.1
)

```
</details>
